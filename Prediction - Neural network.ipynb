{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38ab969",
   "metadata": {},
   "source": [
    "# Prediction using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae076e",
   "metadata": {},
   "source": [
    "In this notebook, we explore the provided data to build intuition on which models to use, which features to retain and more generally on the data challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff37037",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os libraries\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd315212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical learning libraries\n",
    "import sklearn.ensemble as ens\n",
    "import sklearn.feature_selection as fs\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.preprocessing as pr\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.neighbors as nb\n",
    "import sklearn.svm as sv\n",
    "import sklearn.neural_network as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural networks libraries\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation libraires\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff5573",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(model, X, y):\n",
    "    '''\n",
    "    Get MSE of model on test data.\n",
    "    \n",
    "    Arguments:\n",
    "        model: prediction model\n",
    "        \n",
    "    Returns:\n",
    "        score: MSE loss\n",
    "    '''\n",
    "    \n",
    "    # compute number of points in data\n",
    "    n = y.shape[0]\n",
    "    \n",
    "    # return loss\n",
    "    return (1/n) * np.sum(np.square(model.predict(X) - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_nn(model, X):\n",
    "    '''\n",
    "    Export results into CSV file for submission.\n",
    "    \n",
    "    Arguments:\n",
    "        model: regression model\n",
    "    '''\n",
    "    \n",
    "    # obtain predictions\n",
    "    pred = model(torch.tensor(X.values).float()).detach().numpy().squeeze()\n",
    "    \n",
    "    # obtain index of data\n",
    "    idx = X.index\n",
    "    \n",
    "    # set in dataframe\n",
    "    df_results = pd.DataFrame({'_ID': idx, '0': pred})\n",
    "    \n",
    "    # save dataframe\n",
    "    df_results.to_csv('submissions/submit.csv', sep=',', index=False, index_label='_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b6bd5",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_train\n",
    "df_X_train = pd.read_csv('data/input_training.csv', sep=',', header=0, index_col=0)\n",
    "X_train = df_X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read y_train\n",
    "df_y_train = pd.read_csv('data/output_training.csv', sep=',', header=0, index_col=0)\n",
    "y_train = df_y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_test\n",
    "df_X_test = pd.read_csv('data/input_testing.csv', sep=',', header=0, index_col=0)\n",
    "X_test = df_X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ef88e",
   "metadata": {},
   "source": [
    "### Data Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test datasets\n",
    "df = pd.concat([df_X_train, df_X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925910c",
   "metadata": {},
   "source": [
    "### Exploration and creation of an augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62be29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create summary train dataset\n",
    "summary = pd.DataFrame(columns=['Mean', 'Standard deviation', 'Range', 'Number of values', 'Values'], index=df.columns)\n",
    "\n",
    "# create Pandas summary train dataset\n",
    "summary_df = df.describe()\n",
    "\n",
    "# compute statistics for each feature\n",
    "for feature in df.columns:\n",
    "    mean = summary_df[feature][1]\n",
    "    std = summary_df[feature][2]\n",
    "    min = summary_df[feature][3]\n",
    "    max = summary_df[feature][7]\n",
    "    values = set(df[feature])\n",
    "    n_values = len(set(values))\n",
    "    \n",
    "    # populate dataset if n_values <= 10\n",
    "    if n_values <= 50:\n",
    "        summary.loc[feature] = pd.Series({'Mean':'{:0.2f}'.format(mean),\\\n",
    "                                          'Standard deviation':'{:0.2f}'.format(std),\\\n",
    "                                          'Range':'[{:0.2f}, {:0.2f}]'.format(min, max),\\\n",
    "                                          'Number of values':'{:0.0f}'.format(n_values),\\\n",
    "                                          'Values':', '.join([\"{:0.2f}\".format(x) for x in sorted(values)])})\n",
    "        \n",
    "    \n",
    "    # populate dataset otherwise\n",
    "    else:\n",
    "        summary.loc[feature] = pd.Series({'Mean':'{:0.2f}'.format(mean),\\\n",
    "                                          'Standard deviation':'{:0.2f}'.format(std),\\\n",
    "                                          'Range':'[{:0.2f}, {:0.2f}]'.format(min, max),\\\n",
    "                                          'Number of values':'{:0.0f}'.format(n_values),\\\n",
    "                                          'Values':'NA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e445051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of categorical features\n",
    "categorical_features = ['X3', 'X6', 'X11', 'X15', 'X16', 'X18', 'X19', 'X22', 'X28', 'X32', 'X33', 'X35', 'X36',\n",
    "                        'X42', 'X49', 'X56', 'X58', 'X60', 'X62', 'X64', 'X68', 'X73', 'X74', 'X83', 'X86', 'X90',\n",
    "                        'X104', 'X108', 'X109', 'X116', 'X117', 'X122', 'X130', 'X137', 'X139', 'X140', 'X141',\n",
    "                        'X143', 'X144', 'X148', 'X149', 'X151', 'X162', 'X168', 'X169', 'X172', 'X174', 'X176',\n",
    "                        'X177', 'X182', 'X184', 'X186', 'X187', 'X192', 'X193', 'X195', 'X196', 'X197', 'X199',\n",
    "                        'X206', 'X209', 'X217', 'X219', 'X222', 'X231', 'X235', 'X238', 'X242', 'X246', 'X256',\n",
    "                        'X260', 'X270', 'X275', 'X281', 'X285', 'X286', 'X291', 'X298', 'X301', 'X303', 'X304',\n",
    "                        'X307', 'X308', 'X312', 'X314', 'X318', 'X330', 'X332', 'X336', 'X337', 'X338']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103009fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of categorical features with exactly two possible values\n",
    "categorical_features_two = summary[summary['Number of values'].astype(int) == 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad690783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of categorical features with strictly more than two possible values\n",
    "categorical_features_more_than_two = [x for x in categorical_features if x not in categorical_features_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create augmented train dataset by one-hot encoding features with strictly more than two possible values\n",
    "df_augmented = df.copy()\n",
    "for feature in categorical_features_more_than_two:\n",
    "    _ = pd.get_dummies(df[feature])\n",
    "    _.columns = [feature+'-'+str(i) for i in range(1, len(_.columns)+1)]\n",
    "    df_augmented = df_augmented.drop(feature, axis = 1)\n",
    "    df_augmented = df_augmented.join(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate to retrieve df_X_train\n",
    "df_X_train_augmented = df_augmented.truncate(before=None, after=df_X_train.shape[0])\n",
    "X_train_augmented = df_X_train_augmented.values\n",
    "\n",
    "# truncate to retrieve df_X_test\n",
    "df_X_test_augmented = df_augmented.truncate(before=df_X_train.shape[0]+1, after=None)\n",
    "X_test_augmented = df_X_test_augmented.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataset\n",
    "Xt, Xv, yt, yv = ms.train_test_split(X_train, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4982c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation augmented dataset\n",
    "Xta, Xva, yta, yva = ms.train_test_split(X_train_augmented, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03790682",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xta = torch.tensor(Xta).float().to(device)\n",
    "yta = torch.tensor(yta.squeeze()).float().to(device)\n",
    "\n",
    "Xva = torch.tensor(Xva).float().to(device)\n",
    "yva = torch.tensor(yva.squeeze()).float().to(device)\n",
    "\n",
    "Xt = torch.tensor(Xt).float().to(device)\n",
    "yt = torch.tensor(yt.squeeze()).float().to(device)\n",
    "\n",
    "Xv = torch.tensor(Xv).float().to(device)\n",
    "yv = torch.tensor(yv.squeeze()).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b204fd0",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c329b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print shape of datasets\n",
    "print('Train data shape:', Xt.shape)\n",
    "print('Train data (augmented) shape:', Xta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c6c51",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, din):\n",
    "        if self.training:\n",
    "            return din + torch.autograd.Variable(torch.randn(din.size()) * self.stddev)\n",
    "        return din"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e07d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, m, sigma, d):\n",
    "        \n",
    "        super(NN, self).__init__()\n",
    "    \n",
    "        self.m = m\n",
    "        self.sigma = sigma\n",
    "        self.d = d\n",
    "    \n",
    "        self.noise1 = GaussianNoise(self.sigma)\n",
    "        \n",
    "        self.linear1 = nn.Linear(480, self.m)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(self.m)\n",
    "        self.relu1 = nn.LeakyReLU(0.1)\n",
    "        self.dropout1 = nn.Dropout(self.d)\n",
    "        \n",
    "        self.linear2 = nn.Linear(self.m, self.m // 2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(self.m // 2)\n",
    "        self.relu2 = nn.LeakyReLU(0.1)\n",
    "        self.dropout2 = nn.Dropout(self.d)\n",
    "        \n",
    "        self.linear3 = nn.Linear(self.m // 2, 10)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(10)\n",
    "        self.relu3 = nn.LeakyReLU(0.1)\n",
    "        self.dropout3 = nn.Dropout(self.d)\n",
    "        \n",
    "        self.linear4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.noise1(x)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.linear4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xta = torch.tensor(X_train_augmented).float().to(device)\n",
    "#yta = torch.tensor(y_train.squeeze()).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4000aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr, n_epochs, verbose=True):\n",
    "        '''\n",
    "        Training function.\n",
    "\n",
    "        Arguments:\n",
    "            model: torchvision.model\n",
    "                - model to train\n",
    "\n",
    "        Returns:\n",
    "            model: torchvision.model\n",
    "                - trained model\n",
    "            losses: dict\n",
    "                - training and validation losses (per epoch)\n",
    "        '''\n",
    "\n",
    "        if verbose == True:\n",
    "            # print function start\n",
    "            print('#'*30)\n",
    "            print('Initialising training \\n')\n",
    "\n",
    "        # training start time\n",
    "        time_start = time.time()\n",
    "\n",
    "        # set loss function\n",
    "        loss_function = nn.MSELoss()\n",
    "\n",
    "        # set optimizer\n",
    "        optimizer = torch.optim.RAdam(model.parameters(), weight_decay=1e-5, lr=lr, betas=[0.9, 0.99])\n",
    "\n",
    "        # intialise lists to store validation and train accuracies and losses\n",
    "        losses = {'train':[], 'validation':[]}\n",
    "\n",
    "        # initialise past validation losses for LR scheduler\n",
    "        past_validation_losses = [np.inf]*10\n",
    "        \n",
    "        if verbose == True:\n",
    "            # print training start\n",
    "            print('#'*30)\n",
    "            print('Starting training\\n')\n",
    "\n",
    "        # iterate over epochs\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # epoch start time\n",
    "            time_start_epoch = time.time()\n",
    "            \n",
    "            if verbose == True:\n",
    "                # print current epoch number\n",
    "                print(('#' * (30)))\n",
    "                print('Starting epoch {}/{}'.format(epoch+1, n_epochs))\n",
    "                print(('-' * (30)))\n",
    "\n",
    "            # iterate over train and validation phases\n",
    "            for phase in ('train', 'validation'):\n",
    "\n",
    "                # set model to training mode\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "\n",
    "                # set model to evaluation mode for validation\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    # compute loss\n",
    "                    if phase == 'train':\n",
    "                        outputs = model(Xta)\n",
    "                        loss = loss_function(outputs.squeeze(), yta)\n",
    "                    \n",
    "                    elif phase == 'validation':\n",
    "                        outputs = model(Xva)\n",
    "                        loss = loss_function(outputs.squeeze(), yva)\n",
    "                    \n",
    "                    # backward\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # print epoch's loss and accuracy at training phase\n",
    "                if phase == \"train\":\n",
    "                    if verbose == True:\n",
    "                        print('Training phase | Loss: {:.5f}'.format(loss.item()))\n",
    "\n",
    "                    # add loss to loss history\n",
    "                    losses['train'].append(loss.item())\n",
    "\n",
    "                # print epoch's loss and accuracy at validation phase, and update learning rate if required\n",
    "                elif phase == 'validation':\n",
    "                    if verbose == True:\n",
    "                        print('Validat. phase | Loss: {:.5f}'.format(loss.item()))\n",
    "\n",
    "                    # change learning rate\n",
    "                    past_validation_losses.append(loss.item())\n",
    "                    past_validation_losses.pop(0)\n",
    "\n",
    "                    # check if validation loss is not decreasing anymore\n",
    "                    if all(i <= past_validation_losses[-1] for i in past_validation_losses):\n",
    "\n",
    "                        # update learning rate\n",
    "                        lr = lr / 2\n",
    "\n",
    "                        # update optimizer\n",
    "                        optimizer = torch.optim.RAdam(model.parameters(), weight_decay=1e-5, lr=lr, betas=[0.9, 0.999])\n",
    "                        \n",
    "                        if verbose == True:\n",
    "                            # printe learning rate update\n",
    "                            print('*'*30)\n",
    "                            print('Learning rate update to: {:.0e}'.format(lr))\n",
    "                            print('*'*30)\n",
    "\n",
    "                        # reset past validation losses\n",
    "                        past_validation_losses = [np.inf]*10\n",
    "\n",
    "                    # add loss to loss history\n",
    "                    losses['validation'].append(loss.item())\n",
    "                    \n",
    "                    if verbose == True:\n",
    "                        # plot losses\n",
    "                        plt.figure(figsize=(12,8))\n",
    "                        plt.plot(range(len(losses['train'])), losses['train'], label = 'Training Loss', color='black', linestyle='dashed')\n",
    "                        plt.plot(range(len(losses['validation'])), losses['validation'], label = 'Validation Loss', color='black')\n",
    "                        plt.legend()\n",
    "                        plt.xlabel('Number of epochs')\n",
    "                        plt.ylabel('Loss')\n",
    "                        plt.savefig('loss.png')\n",
    "                        plt.close()\n",
    "\n",
    "            if verbose == True:\n",
    "                # print time since start of epoch\n",
    "                time_end_epoch = time.time()\n",
    "                time_epoch = time_end_epoch - time_start_epoch\n",
    "                print(('-' * (30)))\n",
    "                print('Epoch complete in {:.0f}m {:.0f}s \\n'.format(time_epoch // 60, time_epoch % 60))\n",
    "\n",
    "        if verbose == True:\n",
    "            # print time since start of epoch\n",
    "            time_end = time.time()\n",
    "            time_training = time_end - time_start\n",
    "            print(('#' * (30)))\n",
    "            print('Training complete in {:.0f}m {:.0f}s'.format(time_training // 60, time_training % 60))\n",
    "\n",
    "        # return model and losses\n",
    "        return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(model, init_method='kaiming'):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if init_method == 'normal':\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                    nn.init.normal_(m.weight.data)\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "        elif init_method == 'xavier':\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                    nn.init.xavier_normal_(m.weight.data)\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "        elif init_method == 'kaiming':\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                    nn.init.kaiming_normal_(m.weight.data)\n",
    "                    m.bias.data.zero_()\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(m=128, sigma=0.2, d=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d689389",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialise(model, init_method='kaiming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea829e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, losses = train(model, lr=0.1, n_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e282209",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_results_nn(model, df_X_test_augmented)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
