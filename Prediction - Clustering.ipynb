{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38ab969",
   "metadata": {},
   "source": [
    "# Prediction using Clustering + Kernel ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae076e",
   "metadata": {},
   "source": [
    "In this notebook, we explore the provided data to build intuition on which models to use, which features to retain and more generally on the data challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff37037",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os libraries\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd315212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical learning libraries\n",
    "import sklearn.preprocessing as pr\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.kernel_ridge as kr\n",
    "import sklearn.cluster as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural networks libraries\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation libraires\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff5573",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(model, X, y):\n",
    "    '''\n",
    "    Get MSE of model on test data.\n",
    "    \n",
    "    Arguments:\n",
    "        model: prediction model\n",
    "        \n",
    "    Returns:\n",
    "        score: MSE loss\n",
    "    '''\n",
    "    \n",
    "    # compute number of points in data\n",
    "    n = y.shape[0]\n",
    "    \n",
    "    # return loss\n",
    "    return (1/n) * np.sum(np.square(model.predict(X) - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(model, X):\n",
    "    '''\n",
    "    Export results into CSV file for submission.\n",
    "    \n",
    "    Arguments:\n",
    "        model: regression model\n",
    "    '''\n",
    "    \n",
    "    # obtain predictions\n",
    "    pred = model.predict(X)\n",
    "    \n",
    "    # obtain index of data\n",
    "    idx = X.index\n",
    "    \n",
    "    # set in dataframe\n",
    "    df_results = pd.DataFrame({'_ID': idx, '0': pred})\n",
    "    \n",
    "    # save dataframe\n",
    "    df_results.to_csv('submissions/submit.csv', sep=',', index=False, index_label='_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b6bd5",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_train\n",
    "df_X_train = pd.read_csv('data/input_training.csv', sep=',', header=0, index_col=0)\n",
    "X_train = df_X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read y_train\n",
    "df_y_train = pd.read_csv('data/output_training.csv', sep=',', header=0, index_col=0)\n",
    "y_train = df_y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_test\n",
    "df_X_test = pd.read_csv('data/input_testing.csv', sep=',', header=0, index_col=0)\n",
    "X_test = df_X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ef88e",
   "metadata": {},
   "source": [
    "### Data Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test datasets\n",
    "df = pd.concat([df_X_train, df_X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925910c",
   "metadata": {},
   "source": [
    "### Exploration and creation of an augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62be29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create summary train dataset\n",
    "summary = pd.DataFrame(columns=['Mean', 'Standard deviation', 'Range', 'Number of values', 'Values'], index=df.columns)\n",
    "\n",
    "# create Pandas summary train dataset\n",
    "summary_df = df.describe()\n",
    "\n",
    "# compute statistics for each feature\n",
    "for feature in df.columns:\n",
    "    mean = summary_df[feature][1]\n",
    "    std = summary_df[feature][2]\n",
    "    min = summary_df[feature][3]\n",
    "    max = summary_df[feature][7]\n",
    "    values = set(df[feature])\n",
    "    n_values = len(set(values))\n",
    "    \n",
    "    # populate dataset if n_values <= 10\n",
    "    if n_values <= 50:\n",
    "        summary.loc[feature] = pd.Series({'Mean':'{:0.2f}'.format(mean),\\\n",
    "                                          'Standard deviation':'{:0.2f}'.format(std),\\\n",
    "                                          'Range':'[{:0.2f}, {:0.2f}]'.format(min, max),\\\n",
    "                                          'Number of values':'{:0.0f}'.format(n_values),\\\n",
    "                                          'Values':', '.join([\"{:0.2f}\".format(x) for x in sorted(values)])})\n",
    "        \n",
    "    \n",
    "    # populate dataset otherwise\n",
    "    else:\n",
    "        summary.loc[feature] = pd.Series({'Mean':'{:0.2f}'.format(mean),\\\n",
    "                                          'Standard deviation':'{:0.2f}'.format(std),\\\n",
    "                                          'Range':'[{:0.2f}, {:0.2f}]'.format(min, max),\\\n",
    "                                          'Number of values':'{:0.0f}'.format(n_values),\\\n",
    "                                          'Values':'NA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e445051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of categorical features\n",
    "categorical_features = ['X3', 'X6', 'X11', 'X15', 'X16', 'X18', 'X19', 'X22', 'X28', 'X32', 'X33', 'X35', 'X36',\n",
    "                        'X42', 'X49', 'X56', 'X58', 'X60', 'X62', 'X64', 'X68', 'X73', 'X74', 'X83', 'X86', 'X90',\n",
    "                        'X104', 'X108', 'X109', 'X116', 'X117', 'X122', 'X130', 'X137', 'X139', 'X140', 'X141',\n",
    "                        'X143', 'X144', 'X148', 'X149', 'X151', 'X162', 'X168', 'X169', 'X172', 'X174', 'X176',\n",
    "                        'X177', 'X182', 'X184', 'X186', 'X187', 'X192', 'X193', 'X195', 'X196', 'X197', 'X199',\n",
    "                        'X206', 'X209', 'X217', 'X219', 'X222', 'X231', 'X235', 'X238', 'X242', 'X246', 'X256',\n",
    "                        'X260', 'X270', 'X275', 'X281', 'X285', 'X286', 'X291', 'X298', 'X301', 'X303', 'X304',\n",
    "                        'X307', 'X308', 'X312', 'X314', 'X318', 'X330', 'X332', 'X336', 'X337', 'X338']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103009fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of categorical features with exactly two possible values\n",
    "categorical_features_two = summary[summary['Number of values'].astype(int) == 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad690783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of categorical features with strictly more than two possible values\n",
    "categorical_features_more_than_two = [x for x in categorical_features if x not in categorical_features_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create augmented train dataset by one-hot encoding features with strictly more than two possible values\n",
    "df_augmented = df.copy()\n",
    "for feature in categorical_features_more_than_two:\n",
    "    _ = pd.get_dummies(df[feature])\n",
    "    _.columns = [feature+'-'+str(i) for i in range(1, len(_.columns)+1)]\n",
    "    df_augmented = df_augmented.drop(feature, axis = 1)\n",
    "    df_augmented = df_augmented.join(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate to retrieve df_X_train\n",
    "df_X_train_augmented = df_augmented.truncate(before=None, after=df_X_train.shape[0])\n",
    "X_train_augmented = df_X_train_augmented.values\n",
    "\n",
    "# truncate to retrieve df_X_test\n",
    "df_X_test_augmented = df_augmented.truncate(before=df_X_train.shape[0]+1, after=None)\n",
    "X_test_augmented = df_X_test_augmented.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataset\n",
    "Xt, Xv, yt, yv = ms.train_test_split(X_train, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4982c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation augmented dataset\n",
    "Xta, Xva, yta, yva = ms.train_test_split(X_train_augmented, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b204fd0",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c329b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print shape of datasets\n",
    "print('Train data shape:', Xt.shape)\n",
    "print('Train data (augmented) shape:', Xta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c6c51",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c973fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise list\n",
    "df_results_list = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "\n",
    "    # compute k-means over dataset\n",
    "    clu = cl.KMeans(n_clusters = k).fit(df_augmented)\n",
    "    \n",
    "    batch_scores = []\n",
    "    \n",
    "    for batch in range(20):\n",
    "    \n",
    "        # create validation augmented dataset\n",
    "        Xta, Xva, yta, yva = ms.train_test_split(df_X_train_augmented, df_y_train, test_size=1/5)\n",
    "\n",
    "        # compute labels\n",
    "        kmeans_labels_train = clu.predict(Xta)\n",
    "        kmeans_labels_test = clu.predict(Xva)\n",
    "\n",
    "        # compute predictions for each cluster\n",
    "        for c in range(k):\n",
    "\n",
    "            # set KernelRidge regressor\n",
    "            krr = kr.KernelRidge(alpha=0.06, kernel = \"laplacian\")\n",
    "\n",
    "            # set train datasets\n",
    "            X_train_k = Xta[kmeans_labels_train == c]\n",
    "            y_train_k = yta[kmeans_labels_train == c]\n",
    "\n",
    "            # set test dataset\n",
    "            X_test_k = Xva[kmeans_labels_test == c]\n",
    "            X_test_idx_k = X_test_k.index\n",
    "            \n",
    "            # compute only if cluster in test set\n",
    "            if X_test_k.shape[0] > 0:\n",
    "\n",
    "                # fit KernelRidge\n",
    "                krr.fit(X_train_k, y_train_k)\n",
    "\n",
    "                # predict cluster\n",
    "                y_pred_k = krr.predict(X_test_k).squeeze()\n",
    "                \n",
    "                # set dataframe\n",
    "                df_y_pred_k = pd.DataFrame({'_ID': X_test_idx_k, 'YP': y_pred_k})\n",
    "\n",
    "                # concatenate to results DataFrame\n",
    "                df_results_list.append(df_y_pred_k)\n",
    "\n",
    "        # create results dataset\n",
    "        df_results = pd.concat(df_results_list)\n",
    "\n",
    "        df_results = df_results.set_index('_ID')\n",
    "\n",
    "        # compute score\n",
    "        df_score = yva.merge(df_results, left_index=True, right_index=True, how='inner')\n",
    "        df_score['S'] = df_score['Y'] - df_score['YP']\n",
    "        \n",
    "        score = (1/df_score['S'].shape[0]) * np.sum(np.square(df_score['S']))\n",
    "        \n",
    "        batch_scores.append(score)\n",
    "        \n",
    "    batch_score = np.mean(batch_scores)\n",
    "    print(k, batch_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e282209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
